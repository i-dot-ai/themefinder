{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to ThemeFinder","text":"<p>ThemeFinder is a topic modelling Python package designed for analyzing one-to-many question-answer data (i.e. survey responses, public consultations, etc.).</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>See README for how to get started with this package.</p>"},{"location":"#themefinder-pipeline","title":"ThemeFinder pipeline","text":"<p>ThemeFinder's pipeline consists of five distinct stages, each utilizing a specialized LLM prompt:</p>"},{"location":"#sentiment-analysis","title":"Sentiment analysis","text":"<ul> <li>Analyses the emotional tone and position of each response using sentiment-focused prompts</li> <li>Provides structured sentiment categorisation based on LLM analysis</li> </ul>"},{"location":"#theme-generation","title":"Theme generation","text":"<ul> <li>Uses exploratory prompts to identify initial themes from response batches</li> <li>Groups related responses for better context through guided theme extraction</li> </ul>"},{"location":"#theme-condensation","title":"Theme condensation","text":"<ul> <li>Employs comparative prompts to combine similar or overlapping themes</li> <li>Reduces redundancy in identified topics through systematic theme evaluation</li> </ul>"},{"location":"#theme-refinement","title":"Theme refinement","text":"<ul> <li>Leverages standardisation prompts to normalise theme descriptions</li> <li>Creates clear, consistent theme definitions through structured refinement</li> </ul>"},{"location":"#theme-target-alignment","title":"Theme target alignment","text":"<ul> <li>Optional step to consolidate themes down to a target number</li> </ul>"},{"location":"#theme-mapping","title":"Theme mapping","text":"<ul> <li>Utilizes classification prompts to map individual responses to refined themes</li> <li>Supports multiple theme assignments per response through detailed analysis</li> </ul> <p>The prompts used at each stage can be found in <code>src/themefinder/prompts/</code>.</p> <p>The file <code>src/themefinder.core.py</code> contains the function <code>find_themes</code> which runs the pipeline. It also contains functions for each individual stage.</p>"},{"location":"api_reference/","title":"API reference","text":"<ul> <li>API reference</li> <li>tasks<ul> <li>cross_cutting_themes</li> <li>detail_detection</li> <li>find_themes</li> <li>theme_clustering</li> <li>theme_condensation</li> <li>theme_generation</li> <li>theme_mapping</li> <li>theme_refinement</li> </ul> </li> </ul>"},{"location":"api_reference/#themefinder.tasks","title":"tasks","text":""},{"location":"api_reference/#themefinder.tasks.cross_cutting_themes","title":"cross_cutting_themes","text":"<pre><code>cross_cutting_themes(questions_themes: dict[int, DataFrame], llm: RunnableWithFallbacks, n_concepts: int = 5, min_themes: int = 5, config: RunnableConfig | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Identify cross-cutting themes using a single-pass agent approach.</p> <p>This function analyzes refined themes from multiple questions to identify semantic patterns that span across different questions, creating cross-cutting theme categories that represent common concerns or policy areas.</p> <p>The analysis uses a single-pass process: 1. Identify high-level cross-cutting themes across all questions 2. Map individual themes to the identified cross-cutting themes 3. Refine descriptions based on assigned themes</p> <p>Parameters:</p> Name Type Description Default <code>questions_themes</code> <code>dict[int, DataFrame]</code> <p>Dictionary mapping question numbers to their refined themes DataFrames. Each DataFrame should have columns: - topic_id: Theme identifier (e.g., 'A', 'B', 'C') - topic: String in format \"topic_name: topic_description\"</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance configured for structured output</p> required <code>n_concepts</code> <code>int</code> <p>The target number of cross-cutting themes to generate</p> <code>5</code> <code>min_themes</code> <code>int</code> <p>Minimum number of themes required for a valid cross-cutting theme group. Groups with fewer themes will be discarded. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing: - DataFrame with cross-cutting themes with columns:     - name: Name of the cross-cutting theme     - description: Description of what this theme represents     - themes: Dictionary mapping question_number to list of theme_keys       e.g., {1: [\"A\", \"B\"], 3: [\"C\"]} - Empty DataFrame (for consistency with other core functions)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If questions_themes is empty or contains invalid data</p> <code>KeyError</code> <p>If required columns are missing from themes DataFrames</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>def cross_cutting_themes(\n    questions_themes: dict[int, pd.DataFrame],\n    llm: RunnableWithFallbacks,\n    n_concepts: int = 5,\n    min_themes: int = 5,\n    config: RunnableConfig | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Identify cross-cutting themes using a single-pass agent approach.\n\n    This function analyzes refined themes from multiple questions to identify semantic\n    patterns that span across different questions, creating cross-cutting theme\n    categories that represent common concerns or policy areas.\n\n    The analysis uses a single-pass process:\n    1. Identify high-level cross-cutting themes across all questions\n    2. Map individual themes to the identified cross-cutting themes\n    3. Refine descriptions based on assigned themes\n\n    Args:\n        questions_themes (dict[int, pd.DataFrame]): Dictionary mapping question numbers\n            to their refined themes DataFrames. Each DataFrame should have columns:\n            - topic_id: Theme identifier (e.g., 'A', 'B', 'C')\n            - topic: String in format \"topic_name: topic_description\"\n        llm (RunnableWithFallbacks): Language model instance configured for\n            structured output\n        n_concepts (int): The target number of cross-cutting themes to generate\n        min_themes (int): Minimum number of themes required for a valid\n            cross-cutting theme group. Groups with fewer themes will be discarded.\n            Defaults to 5.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n            - DataFrame with cross-cutting themes with columns:\n                - name: Name of the cross-cutting theme\n                - description: Description of what this theme represents\n                - themes: Dictionary mapping question_number to list of theme_keys\n                  e.g., {1: [\"A\", \"B\"], 3: [\"C\"]}\n            - Empty DataFrame (for consistency with other core functions)\n\n    Raises:\n        ValueError: If questions_themes is empty or contains invalid data\n        KeyError: If required columns are missing from themes DataFrames\n    \"\"\"\n    # Validate input\n    if not questions_themes:\n        raise ValueError(\"questions_themes cannot be empty\")\n\n    # Use the CrossCuttingThemesAgent with external prompt files\n    agent = CrossCuttingThemesAgent(\n        llm=llm, questions_themes=questions_themes, n_concepts=n_concepts, config=config\n    )\n\n    # Run the analysis\n    agent.analyze()\n\n    # Get results as DataFrame using the agent's method\n    df_results = agent.get_results_as_dataframe()\n\n    # Apply minimum themes filter\n    if min_themes &gt; 0:\n        df_results = df_results[df_results[\"n_themes\"] &gt;= min_themes]\n\n    # Create and return DataFrame with empty unprocessed data for consistency\n    return df_results.reset_index(drop=True), pd.DataFrame()\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.detail_detection","title":"detail_detection  <code>async</code>","text":"<pre><code>detail_detection(responses_df: DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 20, prompt_template: str | Path | PromptTemplate = 'detail_detection', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10, config: RunnableConfig | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Identify responses that provide high-value detailed evidence.</p> <p>This function processes survey responses in batches to analyze their level of detail and evidence using a language model. It identifies responses that contain specific examples, data, or detailed reasoning that provide strong supporting evidence.</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses to analyze. Must contain 'response_id' and 'response' columns.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for detail detection.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of responses to process in each batch. Defaults to 20.</p> <code>20</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"detail_detection\".</p> <code>'detail_detection'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Note <p>The function uses response_id_integrity_check to ensure responses maintain their original order and association after processing.</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>async def detail_detection(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 20,\n    prompt_template: str | Path | PromptTemplate = \"detail_detection\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n    config: RunnableConfig | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Identify responses that provide high-value detailed evidence.\n\n    This function processes survey responses in batches to analyze their level of detail\n    and evidence using a language model. It identifies responses that contain specific\n    examples, data, or detailed reasoning that provide strong supporting evidence.\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses to analyze.\n            Must contain 'response_id' and 'response' columns.\n        llm (RunnableWithFallbacks): Language model instance to use for detail detection.\n        question (str): The survey question.\n        batch_size (int, optional): Number of responses to process in each batch.\n            Defaults to 20.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"detail_detection\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    Note:\n        The function uses response_id_integrity_check to ensure responses maintain\n        their original order and association after processing.\n    \"\"\"\n    logger.info(f\"Running detail detection on {len(responses_df)} responses\")\n    detailed, _ = await batch_and_run(\n        responses_df,\n        prompt_template,\n        llm.with_structured_output(DetailDetectionResponses),\n        batch_size=batch_size,\n        question=question,\n        integrity_check=True,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n    return detailed, _\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.find_themes","title":"find_themes  <code>async</code>","text":"<pre><code>find_themes(responses_df: DataFrame, llm: RunnableWithFallbacks, question: str, system_prompt: str = CONSULTATION_SYSTEM_PROMPT, verbose: bool = True, concurrency: int = 10, config: RunnableConfig | None = None) -&gt; dict[str, str | pd.DataFrame]\n</code></pre> <p>Process survey responses through a multi-stage theme analysis pipeline.</p> <p>This pipeline performs sequential analysis steps: 1. Initial theme generation 2. Theme condensation (combining similar themes) 3. Theme refinement 4. Mapping responses to refined themes 5. Detail detection</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance for text analysis</p> required <code>question</code> <code>str</code> <p>The survey question</p> required <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behaviour. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>verbose</code> <code>bool</code> <p>Whether to show information messages during processing. Defaults to True.</p> <code>True</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <code>config</code> <code>RunnableConfig | None</code> <p>Optional LangChain config for tracing/callbacks. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str | DataFrame]</code> <p>dict[str, str | pd.DataFrame]: Dictionary containing results from each pipeline stage: - question: The survey question string - themes: DataFrame with the final themes output - mapping: DataFrame mapping responses to final themes - detailed_responses: DataFrame with detail detection results - unprocessables: DataFrame containing the inputs that could not be processed by the LLM</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>async def find_themes(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    verbose: bool = True,\n    concurrency: int = 10,\n    config: RunnableConfig | None = None,\n) -&gt; dict[str, str | pd.DataFrame]:\n    \"\"\"Process survey responses through a multi-stage theme analysis pipeline.\n\n    This pipeline performs sequential analysis steps:\n    1. Initial theme generation\n    2. Theme condensation (combining similar themes)\n    3. Theme refinement\n    4. Mapping responses to refined themes\n    5. Detail detection\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses\n        llm (RunnableWithFallbacks): Language model instance for text analysis\n        question (str): The survey question\n        system_prompt (str): System prompt to guide the LLM's behaviour.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        verbose (bool): Whether to show information messages during processing.\n            Defaults to True.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n        config (RunnableConfig | None): Optional LangChain config for tracing/callbacks.\n            Defaults to None.\n\n    Returns:\n        dict[str, str | pd.DataFrame]: Dictionary containing results from each pipeline stage:\n            - question: The survey question string\n            - themes: DataFrame with the final themes output\n            - mapping: DataFrame mapping responses to final themes\n            - detailed_responses: DataFrame with detail detection results\n            - unprocessables: DataFrame containing the inputs that could not be processed by the LLM\n    \"\"\"\n    logger.setLevel(logging.INFO if verbose else logging.CRITICAL)\n\n    theme_df, _ = await theme_generation(\n        responses_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n    condensed_theme_df, _ = await theme_condensation(\n        theme_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n    refined_theme_df, _ = await theme_refinement(\n        condensed_theme_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n\n    mapping_df, mapping_unprocessables = await theme_mapping(\n        responses_df[[\"response_id\", \"response\"]],\n        llm,\n        question=question,\n        refined_themes_df=refined_theme_df,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n    detailed_df, _ = await detail_detection(\n        responses_df[[\"response_id\", \"response\"]],\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n\n    logger.info(\"Finished finding themes\")\n    logger.info(\"Provide feedback or report bugs: packages@cabinetoffice.gov.uk\")\n    return {\n        \"question\": question,\n        \"themes\": refined_theme_df,\n        \"mapping\": mapping_df,\n        \"detailed_responses\": detailed_df,\n        \"unprocessables\": mapping_unprocessables,\n    }\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.theme_clustering","title":"theme_clustering","text":"<pre><code>theme_clustering(themes_df: DataFrame, llm: RunnableWithFallbacks, max_iterations: int = 5, target_themes: int = 10, significance_percentage: float = 10.0, return_all_themes: bool = False, system_prompt: str = CONSULTATION_SYSTEM_PROMPT, config: RunnableConfig | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Perform hierarchical clustering of themes using an agentic approach.</p> <p>This function takes a DataFrame of themes and uses the ThemeClusteringAgent to iteratively merge similar themes into a hierarchical structure, then selects the most significant themes based on a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>themes_df</code> <code>DataFrame</code> <p>DataFrame containing themes with columns: - topic_id: Unique identifier for each theme - topic_label: Short descriptive label for the theme - topic_description: Detailed description of the theme - source_topic_count: Number of source responses for this theme</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance configured with structured output for HierarchicalClusteringResponse</p> required <code>max_iterations</code> <code>int</code> <p>Maximum number of clustering iterations. Defaults to 5.</p> <code>5</code> <code>target_themes</code> <code>int</code> <p>Target number of themes to cluster down to. Defaults to 10.</p> <code>10</code> <code>significance_percentage</code> <code>float</code> <p>Percentage threshold for selecting significant themes. Defaults to 10.0.</p> <code>10.0</code> <code>return_all_themes</code> <code>bool</code> <p>If True, returns all clustered themes. If False, returns only significant themes. Defaults to False.</p> <code>False</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:     - DataFrame of clustered themes (all or significant based on return_all_themes)     - Empty DataFrame (for consistency with other functions)</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>def theme_clustering(\n    themes_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    max_iterations: int = 5,\n    target_themes: int = 10,\n    significance_percentage: float = 10.0,\n    return_all_themes: bool = False,\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    config: RunnableConfig | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Perform hierarchical clustering of themes using an agentic approach.\n\n    This function takes a DataFrame of themes and uses the ThemeClusteringAgent\n    to iteratively merge similar themes into a hierarchical structure, then\n    selects the most significant themes based on a threshold.\n\n    Args:\n        themes_df (pd.DataFrame): DataFrame containing themes with columns:\n            - topic_id: Unique identifier for each theme\n            - topic_label: Short descriptive label for the theme\n            - topic_description: Detailed description of the theme\n            - source_topic_count: Number of source responses for this theme\n        llm (RunnableWithFallbacks): Language model instance configured with\n            structured output for HierarchicalClusteringResponse\n        max_iterations (int, optional): Maximum number of clustering iterations.\n            Defaults to 5.\n        target_themes (int, optional): Target number of themes to cluster down to.\n            Defaults to 10.\n        significance_percentage (float, optional): Percentage threshold for\n            selecting significant themes. Defaults to 10.0.\n        return_all_themes (bool, optional): If True, returns all clustered themes.\n            If False, returns only significant themes. Defaults to False.\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing:\n                - DataFrame of clustered themes (all or significant based on return_all_themes)\n                - Empty DataFrame (for consistency with other functions)\n    \"\"\"\n    logger.info(f\"Starting hierarchical clustering of {len(themes_df)} themes\")\n\n    # Convert DataFrame to ThemeNode objects\n    initial_themes = [\n        ThemeNode(\n            topic_id=row[\"topic_id\"],\n            topic_label=row[\"topic_label\"],\n            topic_description=row[\"topic_description\"],\n            source_topic_count=row[\"source_topic_count\"],\n        )\n        for _, row in themes_df.iterrows()\n    ]\n\n    # Initialise clustering agent with structured output LLM\n    agent = ThemeClusteringAgent(\n        llm.with_structured_output(HierarchicalClusteringResponse),\n        initial_themes,\n        system_prompt,\n        target_themes,\n        config=config,\n    )\n\n    # Perform clustering\n    logger.info(\n        f\"Clustering themes with max_iterations={max_iterations}, target_themes={target_themes}\"\n    )\n    all_themes_df = agent.cluster_themes(\n        max_iterations=max_iterations, target_themes=target_themes\n    )\n\n    # Return appropriate themes based on parameter\n    if return_all_themes:\n        logger.info(\n            f\"Clustering complete: returning all {len(all_themes_df)} clustered themes\"\n        )\n        return all_themes_df, pd.DataFrame()\n    else:\n        # Select significant themes\n        logger.info(\n            f\"Selecting themes with significance_percentage={significance_percentage}%\"\n        )\n        selected_themes_df = agent.select_themes(significance_percentage)\n        logger.info(\n            f\"Clustering complete: returning {len(selected_themes_df)} significant themes\"\n        )\n        return selected_themes_df, pd.DataFrame()\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.theme_condensation","title":"theme_condensation  <code>async</code>","text":"<pre><code>theme_condensation(themes_df: DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 75, prompt_template: str | Path | PromptTemplate = 'theme_condensation', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10, config: RunnableConfig | None = None, **kwargs) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Condense and combine similar themes identified from survey responses.</p> <p>This function processes the initially identified themes to combine similar or overlapping topics into more cohesive, broader categories using an LLM.</p> <p>When the theme count exceeds the batch size, a first pass condenses within each batch independently, then a second pass merges across batches. The model decides organically how many themes to produce \u2014 there is no artificial target.</p> <p>Parameters:</p> Name Type Description Default <code>themes_df</code> <code>DataFrame</code> <p>DataFrame containing the initial themes identified from survey responses.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme condensation.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of themes to process in each batch. Defaults to 75.</p> <code>75</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_condensation\".</p> <code>'theme_condensation'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>async def theme_condensation(\n    themes_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 75,\n    prompt_template: str | Path | PromptTemplate = \"theme_condensation\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n    config: RunnableConfig | None = None,\n    **kwargs,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Condense and combine similar themes identified from survey responses.\n\n    This function processes the initially identified themes to combine similar or\n    overlapping topics into more cohesive, broader categories using an LLM.\n\n    When the theme count exceeds the batch size, a first pass condenses within\n    each batch independently, then a second pass merges across batches. The model\n    decides organically how many themes to produce \u2014 there is no artificial target.\n\n    Args:\n        themes_df (pd.DataFrame): DataFrame containing the initial themes identified\n            from survey responses.\n        llm (RunnableWithFallbacks): Language model instance to use for theme condensation.\n        question (str): The survey question.\n        batch_size (int, optional): Number of themes to process in each batch.\n            Defaults to 75.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_condensation\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    \"\"\"\n    logger.info(f\"Running theme condensation on {len(themes_df)} themes\")\n    themes_df[\"response_id\"] = themes_df.index + 1\n\n    target = 30\n    retry = 0\n    while len(themes_df) &gt; target:\n        original_theme_count = len(themes_df)\n        logger.info(\n            f\"{len(themes_df)} larger than {target}, using recursive theme condensation\"\n        )\n        themes_df, _ = await batch_and_run(\n            themes_df,\n            prompt_template,\n            llm.with_structured_output(ThemeCondensationResponses),\n            batch_size=batch_size,\n            question=question,\n            system_prompt=system_prompt,\n            concurrency=concurrency,\n            config=config,\n            **kwargs,\n        )\n        themes_df = themes_df.sample(frac=1).reset_index(drop=True)\n        themes_df[\"response_id\"] = themes_df.index + 1\n\n        if len(themes_df) == original_theme_count:\n            retry += 1\n            if retry &gt; 1:\n                logger.warning(\"failed to reduce the number of themes after 1 retry\")\n                break\n        else:\n            retry = 0\n\n    themes_df, _ = await batch_and_run(\n        themes_df,\n        prompt_template,\n        llm.with_structured_output(ThemeCondensationResponses),\n        batch_size=batch_size,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n        **kwargs,\n    )\n\n    logger.info(f\"Final number of condensed themes: {themes_df.shape[0]}\")\n    return themes_df, _\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.theme_generation","title":"theme_generation  <code>async</code>","text":"<pre><code>theme_generation(responses_df: DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 50, partition_key: str | None = None, prompt_template: str | Path | PromptTemplate = 'theme_generation', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10, config: RunnableConfig | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Generate themes from survey responses using an LLM.</p> <p>This function processes batches of survey responses to identify common themes or topics.</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses. Must include 'response_id' and 'response' columns.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme generation.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of responses to process in each batch. Defaults to 50.</p> <code>50</code> <code>partition_key</code> <code>str | None</code> <p>Column name to use for batching related responses together. Defaults to None for sequential batching, but can be set to another column name for different grouping strategies.</p> <code>None</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_generation\".</p> <code>'theme_generation'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>async def theme_generation(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 50,\n    partition_key: str | None = None,\n    prompt_template: str | Path | PromptTemplate = \"theme_generation\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n    config: RunnableConfig | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Generate themes from survey responses using an LLM.\n\n    This function processes batches of survey responses to identify common themes or topics.\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses.\n            Must include 'response_id' and 'response' columns.\n        llm (RunnableWithFallbacks): Language model instance to use for theme generation.\n        question (str): The survey question.\n        batch_size (int, optional): Number of responses to process in each batch.\n            Defaults to 50.\n        partition_key (str | None, optional): Column name to use for batching related\n            responses together. Defaults to None for sequential batching, but can be set\n            to another column name for different grouping strategies.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_generation\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    \"\"\"\n    logger.info(f\"Running theme generation on {len(responses_df)} responses\")\n    generated_themes, _ = await batch_and_run(\n        responses_df,\n        prompt_template,\n        llm.with_structured_output(ThemeGenerationResponses),\n        batch_size=batch_size,\n        partition_key=partition_key,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n    return generated_themes, _\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.theme_mapping","title":"theme_mapping  <code>async</code>","text":"<pre><code>theme_mapping(responses_df: DataFrame, llm: RunnableWithFallbacks, question: str, refined_themes_df: DataFrame, batch_size: int = 20, prompt_template: str | Path | PromptTemplate = 'theme_mapping', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10, config: RunnableConfig | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Map survey responses to refined themes using an LLM.</p> <p>This function analyzes each survey response and determines which of the refined themes best matches its content. Multiple themes can be assigned to a single response.</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses. Must include 'response_id' and 'response' columns.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme mapping.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>refined_themes_df</code> <code>DataFrame</code> <p>Single-row DataFrame where each column represents a theme (from theme_refinement stage).</p> required <code>batch_size</code> <code>int</code> <p>Number of responses to process in each batch. Defaults to 20.</p> <code>20</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_mapping\".</p> <code>'theme_mapping'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>async def theme_mapping(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    refined_themes_df: pd.DataFrame,\n    batch_size: int = 20,\n    prompt_template: str | Path | PromptTemplate = \"theme_mapping\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n    config: RunnableConfig | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Map survey responses to refined themes using an LLM.\n\n    This function analyzes each survey response and determines which of the refined\n    themes best matches its content. Multiple themes can be assigned to a single response.\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses.\n            Must include 'response_id' and 'response' columns.\n        llm (RunnableWithFallbacks): Language model instance to use for theme mapping.\n        question (str): The survey question.\n        refined_themes_df (pd.DataFrame): Single-row DataFrame where each column\n            represents a theme (from theme_refinement stage).\n        batch_size (int, optional): Number of responses to process in each batch.\n            Defaults to 20.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_mapping\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    \"\"\"\n    logger.info(\n        f\"Running theme mapping on {len(responses_df)} responses using {len(refined_themes_df)} themes\"\n    )\n\n    def transpose_refined_themes(refined_themes: pd.DataFrame):\n        \"\"\"Transpose topics for increased legibility.\"\"\"\n        transposed_df = pd.DataFrame(\n            [refined_themes[\"topic\"].to_numpy()], columns=refined_themes[\"topic_id\"]\n        )\n        return transposed_df\n\n    mapping, unprocessable = await batch_and_run(\n        responses_df,\n        prompt_template,\n        llm.with_structured_output(ThemeMappingResponses),\n        batch_size=batch_size,\n        question=question,\n        refined_themes=transpose_refined_themes(refined_themes_df).to_dict(\n            orient=\"records\"\n        ),\n        integrity_check=True,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n    return mapping, unprocessable\n</code></pre>"},{"location":"api_reference/#themefinder.tasks.theme_refinement","title":"theme_refinement  <code>async</code>","text":"<pre><code>theme_refinement(condensed_themes_df: DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 10000, prompt_template: str | Path | PromptTemplate = 'theme_refinement', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10, config: RunnableConfig | None = None) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Refine and standardise condensed themes using an LLM.</p> <p>This function processes previously condensed themes to create clear, standardised theme descriptions. It also transforms the output format for improved readability by transposing the results into a single-row DataFrame where columns represent individual themes.</p> <p>Parameters:</p> Name Type Description Default <code>condensed_themes</code> <code>DataFrame</code> <p>DataFrame containing the condensed themes from the previous pipeline stage.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme refinement.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of themes to process in each batch. Defaults to 10000.</p> <code>10000</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_refinement\".</p> <code>'theme_refinement'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Note <p>The function adds sequential response_ids to the input DataFrame and transposes the output for improved readability and easier downstream processing.</p> Source code in <code>src/themefinder/tasks.py</code> <pre><code>async def theme_refinement(\n    condensed_themes_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 10000,\n    prompt_template: str | Path | PromptTemplate = \"theme_refinement\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n    config: RunnableConfig | None = None,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Refine and standardise condensed themes using an LLM.\n\n    This function processes previously condensed themes to create clear, standardised\n    theme descriptions. It also transforms the output format for improved readability\n    by transposing the results into a single-row DataFrame where columns represent\n    individual themes.\n\n    Args:\n        condensed_themes (pd.DataFrame): DataFrame containing the condensed themes\n            from the previous pipeline stage.\n        llm (RunnableWithFallbacks): Language model instance to use for theme refinement.\n        question (str): The survey question.\n        batch_size (int, optional): Number of themes to process in each batch.\n            Defaults to 10000.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_refinement\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    Note:\n        The function adds sequential response_ids to the input DataFrame and\n        transposes the output for improved readability and easier downstream\n        processing.\n    \"\"\"\n    logger.info(f\"Running theme refinement on {len(condensed_themes_df)} responses\")\n    condensed_themes_df[\"response_id\"] = condensed_themes_df.index + 1\n\n    refined_themes, _ = await batch_and_run(\n        condensed_themes_df,\n        prompt_template,\n        llm.with_structured_output(ThemeRefinementResponses),\n        batch_size=batch_size,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        config=config,\n    )\n\n    def assign_sequential_topic_ids(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Assigns sequential alphabetic topic_ids (A, B, ..., Z, AA, AB, ...) to the DataFrame.\n        \"\"\"\n\n        def alpha_ids(n: int) -&gt; list[str]:\n            ids = []\n            for i in range(n):\n                s = \"\"\n                x = i\n                while True:\n                    x, r = divmod(x, 26)\n                    s = chr(65 + r) + s\n                    if x == 0:\n                        break\n                    x -= 1\n                ids.append(s)\n            return ids\n\n        if not df.empty:\n            df[\"topic_id\"] = alpha_ids(len(df))\n        return df\n\n    refined_themes = assign_sequential_topic_ids(refined_themes)\n\n    return refined_themes, _\n</code></pre>"},{"location":"internal_contributors/","title":"Internal i.AI contributors to ThemeFinder","text":"<p>At present we are not accepting contributions to <code>themefinder</code> as it is undergoing rapid development within i.AI. If you have a suggestion, please raise an issue or email us at packages@cabinetoffice.gov.uk.</p>"},{"location":"internal_contributors/#architecture-decisions","title":"Architecture decisions","text":""},{"location":"internal_contributors/#developing-the-package-locally","title":"Developing the package locally","text":"<p>Ensure you have installed <code>pre-commit</code> in the repo: <code>pre-commit install</code>. This includes hooks to prevent you committing sensitive data. This will not catch everything; always take care when working in the open.</p> <p>We use <code>poetry</code> to develop the <code>themefinder</code> - <code>poetry install</code> to install packages.</p> <p>If you wish to install your local development version of the package into another project to test it, you will need to install in \"editable\" mode: <pre><code>pip install -e &lt;FILE_PATH&gt;\n</code></pre> or  <pre><code>poetry add -e &lt;FILE_PATH&gt;\n</code></pre> where <code>&lt;FILE_PATH&gt;</code> is the location of your local version of <code>themefinder</code>.</p>"},{"location":"internal_contributors/#evaluation","title":"Evaluation","text":"<p>The <code>evals/</code> directory contains our benchmarking evaluation suite used to measure system performance. When you make a change to the <code>themefinder</code> pipeline, run the evaluations to ensure you haven't reduced performance. </p> <p>The <code>eval_mapping</code> and <code>eval_sentiment</code> evaluations use sensitive data stored in our AWS environment. These specific evaluations will only function with proper AWS account access and credentials. Similarly, the <code>make run_evals</code> command assumes you have AWS access configured.</p> <p>These evaluations use the Azure Open AI endpoint.</p>"},{"location":"internal_contributors/#running-the-evaluations","title":"Running the evaluations","text":"<p>Set your environment variables: copy <code>.env.example</code> to <code>.env</code> and populate with the name of the S3 bucket, and the details for the Azure endpoint.</p> <p>Install packages for this repo: <code>poetry install</code>.</p> <p>Ensure you have AWS access set up, and assume your AWS role to allow you to access the data.</p> <p>These evaluations can be executed either: - By running <code>make run_evals</code> to execute the complete evaluation suite (or <code>poetry run make run_evals</code> if you're using <code>poetry</code>) - By directly running individual evaluation files that begin with <code>eval_</code> prefix</p> <p>Note that the evals specifically use GPT-4o, and JSON structured output.</p>"},{"location":"internal_contributors/#releasing-to-pypi","title":"Releasing to PyPi","text":"<p>Creating a GitHub release will push that version of the package to TestPyPi and then PyPi.</p> <ol> <li>Check with the Consult engineering team in Slack that it is ok to do a release.</li> <li>Update the version number in <code>pyproject.toml</code> - note that we are using SemVer.</li> <li>From the <code>main</code> branch, create a pre-release by ticking the box at the bottom of the release. The release should have the tag <code>vX.Y.Z</code> where <code>X.Y.Z</code> is the version number in <code>pyproject.toml</code>.</li> <li>Use the \"Generate release notes\" button to get you started on writing a suitable release note.</li> <li>Creating the pre-release should trigger a deployment to TestPyPi. Check the GitHub Actions and TestPyPi to ensure that this happens.</li> <li>Once you're happy, go back to the pre-release and turn it into a release.</li> <li>When you publish the release, this will trigger a deployment to PyPi. You can check the GitHub actions and PyPi itself to confirm the deployment has worked.</li> </ol>"},{"location":"internal_contributors/#docs","title":"Docs","text":"<p>These docs are built using MkDocs, and are deployed to GitHub Pages when a pull request is merged to the <code>main</code> branch. The docs are stored in the <code>docs</code> folder as Markdown files.</p> <p>To test your changes locally: <pre><code>poetry run mkdocs build\npoetry run mkdocs serve\n</code></pre> and go to http://127.0.0.1:8000/i-dot-ai/themefinder/ in the browser.</p>"},{"location":"internal_contributors/#architecture-decision-records-adr","title":"Architecture decision records (ADR)","text":"<p>If you are making significant changes, please record them in the architecure documents. We are using adr-tools - see the ADR tools repo for how to install and use.</p>"},{"location":"internal_contributors/#langfuse-integration","title":"Langfuse integration","text":"<p>Langfuse can be used to monitor costs and log LLM calls. This can be initialised from outside of the ThemeFinder package when the LLM is instantiated. </p> <p>Set up: Add to the <code>.env</code> file: <pre><code># Langfuse\nLANGFUSE_SECRET_KEY=\"\"\nLANGFUSE_PUBLIC_KEY=\"\"\nLANGFUSE_HOST=\"\"\n</code></pre> using the keys from you own langfuse instance.</p> <pre><code>from langfuse import Langfuse\nfrom langfuse.callback import CallbackHandler\n\ndotenv.load_dotenv() \n\n# Initialize Langfuse CallbackHandler for Langchain (tracing)\n# Use the session id to group calls\nlangfuse_callback_handler = CallbackHandler(session_id=\"run_1\")\n\n# Initialise your LLM of choice using langchain\nllm = AzureChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0,\n    callbacks=[langfuse_callback_handler],\n    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n)\n</code></pre> <p>Use as you would normally. Your langfuse dashboard should log the llm calls including the inputs, outputs, model name, costs, and more.</p>"}]}