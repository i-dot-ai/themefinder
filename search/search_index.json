{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ThemeFinder","text":"<p>ThemeFinder is a topic modelling Python package designed for analyzing one-to-many question-answer data (i.e. survey responses, public consultations, etc.).</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>See README for how to get started with this package.</p>"},{"location":"#themefinder-pipeline","title":"ThemeFinder pipeline","text":"<p>ThemeFinder's pipeline consists of five distinct stages, each utilizing a specialized LLM prompt:</p>"},{"location":"#sentiment-analysis","title":"Sentiment analysis","text":"<ul> <li>Analyses the emotional tone and position of each response using sentiment-focused prompts</li> <li>Provides structured sentiment categorisation based on LLM analysis</li> </ul>"},{"location":"#theme-generation","title":"Theme generation","text":"<ul> <li>Uses exploratory prompts to identify initial themes from response batches</li> <li>Groups related responses for better context through guided theme extraction</li> </ul>"},{"location":"#theme-condensation","title":"Theme condensation","text":"<ul> <li>Employs comparative prompts to combine similar or overlapping themes</li> <li>Reduces redundancy in identified topics through systematic theme evaluation</li> </ul>"},{"location":"#theme-refinement","title":"Theme refinement","text":"<ul> <li>Leverages standardisation prompts to normalise theme descriptions</li> <li>Creates clear, consistent theme definitions through structured refinement</li> </ul>"},{"location":"#theme-target-alignment","title":"Theme target alignment","text":"<ul> <li>Optional step to consolidate themes down to a target number</li> </ul>"},{"location":"#theme-mapping","title":"Theme mapping","text":"<ul> <li>Utilizes classification prompts to map individual responses to refined themes</li> <li>Supports multiple theme assignments per response through detailed analysis</li> </ul> <p>The prompts used at each stage can be found in <code>src/themefinder/prompts/</code>.</p> <p>The file <code>src/themefinder.core.py</code> contains the function <code>find_themes</code> which runs the pipeline. It also contains functions for each individual stage.</p>"},{"location":"api_reference/","title":"API reference","text":"<ul> <li>API reference</li> <li>find_themes</li> <li>sentiment_analysis</li> <li>theme_generation</li> <li>theme_condensation</li> <li>theme_refinement</li> <li>theme_target_alignment</li> <li>theme_mapping</li> </ul>"},{"location":"api_reference/#themefinder.core.find_themes","title":"find_themes  <code>async</code>","text":"<pre><code>find_themes(responses_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, target_n_themes: int | None = None, system_prompt: str = CONSULTATION_SYSTEM_PROMPT, verbose: bool = True, concurrency: int = 10) -&gt; dict[str, str | pd.DataFrame]\n</code></pre> <p>Process survey responses through a multi-stage theme analysis pipeline.</p> <p>This pipeline performs sequential analysis steps: 1. Sentiment analysis of responses 2. Initial theme generation 3. Theme condensation (combining similar themes) 4. Theme refinement 5. Theme target alignment (optional, if target_n_themes is specified) 6. Mapping responses to refined themes</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance for text analysis</p> required <code>question</code> <code>str</code> <p>The survey question</p> required <code>target_n_themes</code> <code>int | None</code> <p>Target number of themes to consolidate to. If None, skip theme target alignment step. Defaults to None.</p> <code>None</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>verbose</code> <code>bool</code> <p>Whether to show information messages during processing. Defaults to True.</p> <code>True</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict[str, str | DataFrame]</code> <p>dict[str, str | pd.DataFrame]: Dictionary containing results from each pipeline stage: - question: The survey question string - sentiment: DataFrame with sentiment analysis results - themes: DataFrame with the final themes output - mapping: DataFrame mapping responses to final themes - unprocessables: Dataframe containing the inputs that could not be processed by the LLM</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def find_themes(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    target_n_themes: int | None = None,\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    verbose: bool = True,\n    concurrency: int = 10,\n) -&gt; dict[str, str | pd.DataFrame]:\n    \"\"\"Process survey responses through a multi-stage theme analysis pipeline.\n\n    This pipeline performs sequential analysis steps:\n    1. Sentiment analysis of responses\n    2. Initial theme generation\n    3. Theme condensation (combining similar themes)\n    4. Theme refinement\n    5. Theme target alignment (optional, if target_n_themes is specified)\n    6. Mapping responses to refined themes\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses\n        llm (RunnableWithFallbacks): Language model instance for text analysis\n        question (str): The survey question\n        target_n_themes (int | None, optional): Target number of themes to consolidate to.\n            If None, skip theme target alignment step. Defaults to None.\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        verbose (bool): Whether to show information messages during processing.\n            Defaults to True.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        dict[str, str | pd.DataFrame]: Dictionary containing results from each pipeline stage:\n            - question: The survey question string\n            - sentiment: DataFrame with sentiment analysis results\n            - themes: DataFrame with the final themes output\n            - mapping: DataFrame mapping responses to final themes\n            - unprocessables: Dataframe containing the inputs that could not be processed by the LLM\n    \"\"\"\n    logger.setLevel(logging.INFO if verbose else logging.CRITICAL)\n\n    sentiment_df, sentiment_unprocessables = await sentiment_analysis(\n        responses_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    theme_df, _ = await theme_generation(\n        sentiment_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    condensed_theme_df, _ = await theme_condensation(\n        theme_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    refined_theme_df, _ = await theme_refinement(\n        condensed_theme_df,\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    if target_n_themes is not None:\n        refined_theme_df, _ = await theme_target_alignment(\n            refined_theme_df,\n            llm,\n            question=question,\n            target_n_themes=target_n_themes,\n            system_prompt=system_prompt,\n            concurrency=concurrency,\n        )\n    mapping_df, mapping_unprocessables = await theme_mapping(\n        sentiment_df[[\"response_id\", \"response\"]],\n        llm,\n        question=question,\n        refined_themes_df=refined_theme_df,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    detailed_df, _ = await detail_detection(\n        responses_df[[\"response_id\", \"response\"]],\n        llm,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n\n    logger.info(\"Finished finding themes\")\n    logger.info(\"Provide feedback or report bugs: packages@cabinetoffice.gov.uk\")\n    return {\n        \"question\": question,\n        \"sentiment\": sentiment_df,\n        \"themes\": refined_theme_df,\n        \"mapping\": mapping_df,\n        \"detailed_responses\": detailed_df,\n        \"unprocessables\": pd.concat([sentiment_unprocessables, mapping_unprocessables]),\n    }\n</code></pre>"},{"location":"api_reference/#themefinder.core.sentiment_analysis","title":"sentiment_analysis  <code>async</code>","text":"<pre><code>sentiment_analysis(responses_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 20, prompt_template: str | Path | PromptTemplate = 'sentiment_analysis', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Perform sentiment analysis on survey responses using an LLM.</p> <p>This function processes survey responses in batches to analyze their sentiment using a language model. It maintains response integrity by checking response IDs.</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses to analyze. Must contain 'response_id' and 'response' columns.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for sentiment analysis.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of responses to process in each batch. Defaults to 20.</p> <code>20</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"sentiment_analysis\".</p> <code>'sentiment_analysis'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Note <p>The function uses integrity_check to ensure responses maintain their original order and association after processing.</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def sentiment_analysis(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 20,\n    prompt_template: str | Path | PromptTemplate = \"sentiment_analysis\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Perform sentiment analysis on survey responses using an LLM.\n\n    This function processes survey responses in batches to analyze their sentiment\n    using a language model. It maintains response integrity by checking response IDs.\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses to analyze.\n            Must contain 'response_id' and 'response' columns.\n        llm (RunnableWithFallbacks): Language model instance to use for sentiment analysis.\n        question (str): The survey question.\n        batch_size (int, optional): Number of responses to process in each batch.\n            Defaults to 20.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"sentiment_analysis\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    Note:\n        The function uses integrity_check to ensure responses maintain\n        their original order and association after processing.\n    \"\"\"\n    logger.info(f\"Running sentiment analysis on {len(responses_df)} responses\")\n    sentiment, unprocessable = await batch_and_run(\n        responses_df,\n        prompt_template,\n        llm.with_structured_output(SentimentAnalysisResponses),\n        batch_size=batch_size,\n        question=question,\n        integrity_check=True,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n\n    return sentiment, unprocessable\n</code></pre>"},{"location":"api_reference/#themefinder.core.theme_generation","title":"theme_generation  <code>async</code>","text":"<pre><code>theme_generation(responses_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 50, partition_key: str | None = 'position', prompt_template: str | Path | PromptTemplate = 'theme_generation', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Generate themes from survey responses using an LLM.</p> <p>This function processes batches of survey responses to identify common themes or topics.</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses. Must include 'response_id' and 'response' columns.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme generation.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of responses to process in each batch. Defaults to 50.</p> <code>50</code> <code>partition_key</code> <code>str | None</code> <p>Column name to use for batching related responses together. Defaults to \"position\" for sentiment-enriched responses, but can be set to None for sequential batching or another column name for different grouping strategies.</p> <code>'position'</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_generation\".</p> <code>'theme_generation'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def theme_generation(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 50,\n    partition_key: str | None = \"position\",\n    prompt_template: str | Path | PromptTemplate = \"theme_generation\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Generate themes from survey responses using an LLM.\n\n    This function processes batches of survey responses to identify common themes or topics.\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses.\n            Must include 'response_id' and 'response' columns.\n        llm (RunnableWithFallbacks): Language model instance to use for theme generation.\n        question (str): The survey question.\n        batch_size (int, optional): Number of responses to process in each batch.\n            Defaults to 50.\n        partition_key (str | None, optional): Column name to use for batching related\n            responses together. Defaults to \"position\" for sentiment-enriched responses,\n            but can be set to None for sequential batching or another column name for\n            different grouping strategies.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_generation\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    \"\"\"\n    logger.info(f\"Running theme generation on {len(responses_df)} responses\")\n    generated_themes, _ = await batch_and_run(\n        responses_df,\n        prompt_template,\n        llm.with_structured_output(ThemeGenerationResponses),\n        batch_size=batch_size,\n        partition_key=partition_key,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    return generated_themes, _\n</code></pre>"},{"location":"api_reference/#themefinder.core.theme_condensation","title":"theme_condensation  <code>async</code>","text":"<pre><code>theme_condensation(themes_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 75, prompt_template: str | Path | PromptTemplate = 'theme_condensation', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10, **kwargs) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Condense and combine similar themes identified from survey responses.</p> <p>This function processes the initially identified themes to combine similar or overlapping topics into more cohesive, broader categories using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>themes_df</code> <code>DataFrame</code> <p>DataFrame containing the initial themes identified from survey responses.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme condensation.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of themes to process in each batch. Defaults to 100.</p> <code>75</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_condensation\".</p> <code>'theme_condensation'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def theme_condensation(\n    themes_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 75,\n    prompt_template: str | Path | PromptTemplate = \"theme_condensation\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n    **kwargs,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Condense and combine similar themes identified from survey responses.\n\n    This function processes the initially identified themes to combine similar or\n    overlapping topics into more cohesive, broader categories using an LLM.\n\n    Args:\n        themes_df (pd.DataFrame): DataFrame containing the initial themes identified\n            from survey responses.\n        llm (RunnableWithFallbacks): Language model instance to use for theme condensation.\n        question (str): The survey question.\n        batch_size (int, optional): Number of themes to process in each batch.\n            Defaults to 100.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_condensation\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    \"\"\"\n    logger.info(f\"Running theme condensation on {len(themes_df)} themes\")\n    themes_df[\"response_id\"] = themes_df.index + 1\n\n    n_themes = themes_df.shape[0]\n    while n_themes &gt; batch_size:\n        logger.info(\n            f\"{n_themes} larger than batch size, using recursive theme condensation\"\n        )\n        themes_df, _ = await batch_and_run(\n            themes_df,\n            prompt_template,\n            llm.with_structured_output(ThemeCondensationResponses),\n            batch_size=batch_size,\n            question=question,\n            system_prompt=system_prompt,\n            concurrency=concurrency,\n            **kwargs,\n        )\n        themes_df = themes_df.sample(frac=1).reset_index(drop=True)\n        themes_df[\"response_id\"] = themes_df.index + 1\n        if len(themes_df) == n_themes:\n            logger.info(\"Themes no longer being condensed\")\n            break\n        n_themes = themes_df.shape[0]\n\n    themes_df, _ = await batch_and_run(\n        themes_df,\n        prompt_template,\n        llm.with_structured_output(ThemeCondensationResponses),\n        batch_size=batch_size,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n        **kwargs,\n    )\n\n    logger.info(f\"Final number of condensed themes: {themes_df.shape[0]}\")\n    return themes_df, _\n</code></pre>"},{"location":"api_reference/#themefinder.core.theme_refinement","title":"theme_refinement  <code>async</code>","text":"<pre><code>theme_refinement(condensed_themes_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, batch_size: int = 10000, prompt_template: str | Path | PromptTemplate = 'theme_refinement', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Refine and standardize condensed themes using an LLM.</p> <p>This function processes previously condensed themes to create clear, standardized theme descriptions. It also transforms the output format for improved readability by transposing the results into a single-row DataFrame where columns represent individual themes.</p> <p>Parameters:</p> Name Type Description Default <code>condensed_themes</code> <code>DataFrame</code> <p>DataFrame containing the condensed themes from the previous pipeline stage.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme refinement.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>batch_size</code> <code>int</code> <p>Number of themes to process in each batch. Defaults to 10000.</p> <code>10000</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_refinement\".</p> <code>'theme_refinement'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Note <p>The function adds sequential response_ids to the input DataFrame and transposes the output for improved readability and easier downstream processing.</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def theme_refinement(\n    condensed_themes_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    batch_size: int = 10000,\n    prompt_template: str | Path | PromptTemplate = \"theme_refinement\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Refine and standardize condensed themes using an LLM.\n\n    This function processes previously condensed themes to create clear, standardized\n    theme descriptions. It also transforms the output format for improved readability\n    by transposing the results into a single-row DataFrame where columns represent\n    individual themes.\n\n    Args:\n        condensed_themes (pd.DataFrame): DataFrame containing the condensed themes\n            from the previous pipeline stage.\n        llm (RunnableWithFallbacks): Language model instance to use for theme refinement.\n        question (str): The survey question.\n        batch_size (int, optional): Number of themes to process in each batch.\n            Defaults to 10000.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_refinement\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    Note:\n        The function adds sequential response_ids to the input DataFrame and\n        transposes the output for improved readability and easier downstream\n        processing.\n    \"\"\"\n    logger.info(f\"Running theme refinement on {len(condensed_themes_df)} responses\")\n    condensed_themes_df[\"response_id\"] = condensed_themes_df.index + 1\n\n    refined_themes, _ = await batch_and_run(\n        condensed_themes_df,\n        prompt_template,\n        llm.with_structured_output(ThemeRefinementResponses),\n        batch_size=batch_size,\n        question=question,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    return refined_themes, _\n</code></pre>"},{"location":"api_reference/#themefinder.core.theme_target_alignment","title":"theme_target_alignment  <code>async</code>","text":"<pre><code>theme_target_alignment(refined_themes_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, target_n_themes: int = 10, batch_size: int = 10000, prompt_template: str | Path | PromptTemplate = 'theme_target_alignment', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Align themes to target number using an LLM.</p> <p>This function processes refined themes to consolidate them into a target number of distinct categories while preserving all significant details and perspectives. It transforms the output format for improved readability by transposing the results into a single-row DataFrame where columns represent individual themes.</p> <p>Parameters:</p> Name Type Description Default <code>refined_themes_df</code> <code>DataFrame</code> <p>DataFrame containing the refined themes from the previous pipeline stage.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme alignment.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>target_n_themes</code> <code>int</code> <p>Target number of themes to consolidate to. Defaults to 10.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Number of themes to process in each batch. Defaults to 10000.</p> <code>10000</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_target_alignment\".</p> <code>'theme_target_alignment'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Note <p>The function adds sequential response_ids to the input DataFrame and transposes the output for improved readability and easier downstream processing.</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def theme_target_alignment(\n    refined_themes_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    target_n_themes: int = 10,\n    batch_size: int = 10000,\n    prompt_template: str | Path | PromptTemplate = \"theme_target_alignment\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Align themes to target number using an LLM.\n\n    This function processes refined themes to consolidate them into a target number of\n    distinct categories while preserving all significant details and perspectives.\n    It transforms the output format for improved readability by transposing the\n    results into a single-row DataFrame where columns represent individual themes.\n\n    Args:\n        refined_themes_df (pd.DataFrame): DataFrame containing the refined themes\n            from the previous pipeline stage.\n        llm (RunnableWithFallbacks): Language model instance to use for theme alignment.\n        question (str): The survey question.\n        target_n_themes (int, optional): Target number of themes to consolidate to.\n            Defaults to 10.\n        batch_size (int, optional): Number of themes to process in each batch.\n            Defaults to 10000.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_target_alignment\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    Note:\n        The function adds sequential response_ids to the input DataFrame and\n        transposes the output for improved readability and easier downstream\n        processing.\n    \"\"\"\n    logger.info(\n        f\"Running theme target alignment on {len(refined_themes_df)} themes compressing to {target_n_themes} themes\"\n    )\n    refined_themes_df[\"response_id\"] = refined_themes_df.index + 1\n    aligned_themes, _ = await batch_and_run(\n        refined_themes_df,\n        prompt_template,\n        llm.with_structured_output(ThemeRefinementResponses),\n        batch_size=batch_size,\n        question=question,\n        system_prompt=system_prompt,\n        target_n_themes=target_n_themes,\n        concurrency=concurrency,\n    )\n    return aligned_themes, _\n</code></pre>"},{"location":"api_reference/#themefinder.core.theme_mapping","title":"theme_mapping  <code>async</code>","text":"<pre><code>theme_mapping(responses_df: pd.DataFrame, llm: RunnableWithFallbacks, question: str, refined_themes_df: pd.DataFrame, batch_size: int = 20, prompt_template: str | Path | PromptTemplate = 'theme_mapping', system_prompt: str = CONSULTATION_SYSTEM_PROMPT, concurrency: int = 10) -&gt; tuple[pd.DataFrame, pd.DataFrame]\n</code></pre> <p>Map survey responses to refined themes using an LLM.</p> <p>This function analyzes each survey response and determines which of the refined themes best matches its content. Multiple themes can be assigned to a single response.</p> <p>Parameters:</p> Name Type Description Default <code>responses_df</code> <code>DataFrame</code> <p>DataFrame containing survey responses. Must include 'response_id' and 'response' columns.</p> required <code>llm</code> <code>RunnableWithFallbacks</code> <p>Language model instance to use for theme mapping.</p> required <code>question</code> <code>str</code> <p>The survey question.</p> required <code>refined_themes_df</code> <code>DataFrame</code> <p>Single-row DataFrame where each column represents a theme (from theme_refinement stage).</p> required <code>batch_size</code> <code>int</code> <p>Number of responses to process in each batch. Defaults to 20.</p> <code>20</code> <code>prompt_template</code> <code>str | Path | PromptTemplate</code> <p>Template for structuring the prompt to the LLM. Can be a string identifier, path to template file, or PromptTemplate instance. Defaults to \"theme_mapping\".</p> <code>'theme_mapping'</code> <code>system_prompt</code> <code>str</code> <p>System prompt to guide the LLM's behavior. Defaults to CONSULTATION_SYSTEM_PROMPT.</p> <code>CONSULTATION_SYSTEM_PROMPT</code> <code>concurrency</code> <code>int</code> <p>Number of concurrent API calls to make. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames:     - The first DataFrame contains the rows that were successfully processed by the LLM     - The second DataFrame contains the rows that could not be processed by the LLM</p> Source code in <code>src/themefinder/core.py</code> <pre><code>async def theme_mapping(\n    responses_df: pd.DataFrame,\n    llm: RunnableWithFallbacks,\n    question: str,\n    refined_themes_df: pd.DataFrame,\n    batch_size: int = 20,\n    prompt_template: str | Path | PromptTemplate = \"theme_mapping\",\n    system_prompt: str = CONSULTATION_SYSTEM_PROMPT,\n    concurrency: int = 10,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Map survey responses to refined themes using an LLM.\n\n    This function analyzes each survey response and determines which of the refined\n    themes best matches its content. Multiple themes can be assigned to a single response.\n\n    Args:\n        responses_df (pd.DataFrame): DataFrame containing survey responses.\n            Must include 'response_id' and 'response' columns.\n        llm (RunnableWithFallbacks): Language model instance to use for theme mapping.\n        question (str): The survey question.\n        refined_themes_df (pd.DataFrame): Single-row DataFrame where each column\n            represents a theme (from theme_refinement stage).\n        batch_size (int, optional): Number of responses to process in each batch.\n            Defaults to 20.\n        prompt_template (str | Path | PromptTemplate, optional): Template for structuring\n            the prompt to the LLM. Can be a string identifier, path to template file,\n            or PromptTemplate instance. Defaults to \"theme_mapping\".\n        system_prompt (str): System prompt to guide the LLM's behavior.\n            Defaults to CONSULTATION_SYSTEM_PROMPT.\n        concurrency (int): Number of concurrent API calls to make. Defaults to 10.\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]:\n            A tuple containing two DataFrames:\n                - The first DataFrame contains the rows that were successfully processed by the LLM\n                - The second DataFrame contains the rows that could not be processed by the LLM\n\n    \"\"\"\n    logger.info(\n        f\"Running theme mapping on {len(responses_df)} responses using {len(refined_themes_df)} themes\"\n    )\n\n    def transpose_refined_themes(refined_themes: pd.DataFrame):\n        \"\"\"Transpose topics for increased legibility.\"\"\"\n        transposed_df = pd.DataFrame(\n            [refined_themes[\"topic\"].to_numpy()], columns=refined_themes[\"topic_id\"]\n        )\n        return transposed_df\n\n    mapping, unprocessable = await batch_and_run(\n        responses_df,\n        prompt_template,\n        llm.with_structured_output(ThemeMappingResponses),\n        batch_size=batch_size,\n        question=question,\n        refined_themes=transpose_refined_themes(refined_themes_df).to_dict(\n            orient=\"records\"\n        ),\n        integrity_check=True,\n        system_prompt=system_prompt,\n        concurrency=concurrency,\n    )\n    return mapping, unprocessable\n</code></pre>"},{"location":"internal_contributors/","title":"Internal i.AI contributors to ThemeFinder","text":"<p>At present we are not accepting contributions to <code>themefinder</code> as it is undergoing rapid development within i.AI. If you have a suggestion, please raise an issue or email us at packages@cabinetoffice.gov.uk.</p>"},{"location":"internal_contributors/#architecture-decisions","title":"Architecture decisions","text":""},{"location":"internal_contributors/#developing-the-package-locally","title":"Developing the package locally","text":"<p>Ensure you have installed <code>pre-commit</code> in the repo: <code>pre-commit install</code>. This includes hooks to prevent you committing sensitive data. This will not catch everything; always take care when working in the open.</p> <p>We use <code>poetry</code> to develop the <code>themefinder</code> - <code>poetry install</code> to install packages.</p> <p>If you wish to install your local development version of the package into another project to test it, you will need to install in \"editable\" mode: <pre><code>pip install -e &lt;FILE_PATH&gt;\n</code></pre> or  <pre><code>poetry add -e &lt;FILE_PATH&gt;\n</code></pre> where <code>&lt;FILE_PATH&gt;</code> is the location of your local version of <code>themefinder</code>.</p>"},{"location":"internal_contributors/#evaluation","title":"Evaluation","text":"<p>The <code>evals/</code> directory contains our benchmarking evaluation suite used to measure system performance. When you make a change to the <code>themefinder</code> pipeline, run the evaluations to ensure you haven't reduced performance. </p> <p>The <code>eval_mapping</code> and <code>eval_sentiment</code> evaluations use sensitive data stored in our AWS environment. These specific evaluations will only function with proper AWS account access and credentials. Similarly, the <code>make run_evals</code> command assumes you have AWS access configured.</p> <p>These evaluations use the Azure Open AI endpoint.</p>"},{"location":"internal_contributors/#running-the-evaluations","title":"Running the evaluations","text":"<p>Set your environment variables: copy <code>.env.example</code> to <code>.env</code> and populate with the name of the S3 bucket, and the details for the Azure endpoint.</p> <p>Install packages for this repo: <code>poetry install</code>.</p> <p>Ensure you have AWS access set up, and assume your AWS role to allow you to access the data.</p> <p>These evaluations can be executed either: - By running <code>make run_evals</code> to execute the complete evaluation suite (or <code>poetry run make run_evals</code> if you're using <code>poetry</code>) - By directly running individual evaluation files that begin with <code>eval_</code> prefix</p> <p>Note that the evals specifically use GPT-4o, and JSON structured output.</p>"},{"location":"internal_contributors/#releasing-to-pypi","title":"Releasing to PyPi","text":"<p>Creating a GitHub release will push that version of the package to TestPyPi and then PyPi.</p> <ol> <li>Check with the Consult engineering team in Slack that it is ok to do a release.</li> <li>Update the version number in <code>pyproject.toml</code> - note that we are using SemVer.</li> <li>From the <code>main</code> branch, create a pre-release by ticking the box at the bottom of the release. The release should have the tag <code>vX.Y.Z</code> where <code>X.Y.Z</code> is the version number in <code>pyproject.toml</code>.</li> <li>Use the \"Generate release notes\" button to get you started on writing a suitable release note.</li> <li>Creating the pre-release should trigger a deployment to TestPyPi. Check the GitHub Actions and TestPyPi to ensure that this happens.</li> <li>Once you're happy, go back to the pre-release and turn it into a release.</li> <li>When you publish the release, this will trigger a deployment to PyPi. You can check the GitHub actions and PyPi itself to confirm the deployment has worked.</li> </ol>"},{"location":"internal_contributors/#docs","title":"Docs","text":"<p>These docs are built using MkDocs, and are deployed to GitHub Pages when a pull request is merged to the <code>main</code> branch. The docs are stored in the <code>docs</code> folder as Markdown files.</p> <p>To test your changes locally: <pre><code>poetry run mkdocs build\npoetry run mkdocs serve\n</code></pre> and go to http://127.0.0.1:8000/i-dot-ai/themefinder/ in the browser.</p>"},{"location":"internal_contributors/#architecture-decision-records-adr","title":"Architecture decision records (ADR)","text":"<p>If you are making significant changes, please record them in the architecure documents. We are using adr-tools - see the ADR tools repo for how to install and use.</p>"},{"location":"internal_contributors/#langfuse-integration","title":"Langfuse integration","text":"<p>Langfuse can be used to monitor costs and log LLM calls. This can be initialised from outside of the ThemeFinder package when the LLM is instantiated. </p> <p>Set up: Add to the <code>.env</code> file: <pre><code># Langfuse\nLANGFUSE_SECRET_KEY=\"\"\nLANGFUSE_PUBLIC_KEY=\"\"\nLANGFUSE_HOST=\"\"\n</code></pre> using the keys from you own langfuse instance.</p> <pre><code>from langfuse import Langfuse\nfrom langfuse.callback import CallbackHandler\n\ndotenv.load_dotenv() \n\n# Initialize Langfuse CallbackHandler for Langchain (tracing)\n# Use the session id to group calls\nlangfuse_callback_handler = CallbackHandler(session_id=\"run_1\")\n\n# Initialise your LLM of choice using langchain\nllm = AzureChatOpenAI(\n    model=\"gpt-4o\",\n    temperature=0,\n    callbacks=[langfuse_callback_handler],\n    model_kwargs={\"response_format\": {\"type\": \"json_object\"}},\n)\n</code></pre> <p>Use as you would normally. Your langfuse dashboard should log the llm calls including the inputs, outputs, model name, costs, and more.</p>"}]}